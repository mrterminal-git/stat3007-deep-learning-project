{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39f9cbc",
   "metadata": {},
   "source": [
    "### This notebook will contain the loading of the datasets, and tie together the different parts of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import DataLoader\n",
    "from models.CNN import *\n",
    "from models.FFN import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.CointegrationResidualGenerator import CointegrationResidualGenerator\n",
    "from models.BacktestSharpeEvaluator import BacktestSharpeEvaluator\n",
    "from PortfolioOptimizer import PortfolioOptimizer\n",
    "from TimeSeriesDataSplitter import TimeSeriesDataSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729c902",
   "metadata": {},
   "source": [
    "## Sharpe ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8397151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio_loss(returns, risk_free_rate=0.0):\n",
    "    \"\"\"\n",
    "    Custom loss function to maximize the Sharpe Ratio.\n",
    "    Args:\n",
    "        returns (torch.Tensor): Predicted returns\n",
    "        risk_free_rate (float): Risk-free rate for Sharpe calculation\n",
    "    Returns:\n",
    "        torch.Tensor: Negative Sharpe Ratio (to minimize)\n",
    "    \"\"\"\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    mean_excess = torch.mean(excess_returns)\n",
    "    std_excess = torch.std(excess_returns, unbiased=False) + 1e-6  # epsilon for stability\n",
    "    sharpe_ratio = mean_excess / std_excess\n",
    "    return -sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab69198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from ../data/european_wholesale_electricity_price_data_daily.csv\n",
      "\n",
      "--- List of Countries ---\n",
      "['Austria', 'Belgium', 'Czechia', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland', 'United Kingdom', 'Bulgaria', 'Serbia', 'Croatia', 'Montenegro', 'North Macedonia', 'Ireland']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/filip/stat7007/project/src/DataLoader.py:143: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  price_matrix = price_matrix.fillna(method=fill_method)\n",
      "/Users/filip/stat7007/project/venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (1506, 31, 30), Train next day returns shape: (1506, 31)\n",
      "Validation dataset shape: (322, 31, 30), Validation next day returns shape: (322, 31)\n",
      "Test dataset shape: (324, 31, 30), Test next day returns shape: (324, 31)\n",
      "Training with filters: 8, filter size: 3, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0189\n",
      "Training with filters: 8, filter size: 3, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: -0.0444\n",
      "Training with filters: 8, filter size: 5, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0053\n",
      "Training with filters: 8, filter size: 5, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.1501\n",
      "Training with filters: 8, filter size: 7, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0003\n",
      "Training with filters: 8, filter size: 7, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0860\n",
      "Training with filters: 16, filter size: 3, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: -0.0619\n",
      "Training with filters: 16, filter size: 3, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0812\n",
      "Training with filters: 16, filter size: 5, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0469\n",
      "Training with filters: 16, filter size: 5, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: -0.0014\n",
      "Training with filters: 16, filter size: 7, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0472\n",
      "Training with filters: 16, filter size: 7, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: -0.0091\n",
      "Training with filters: 32, filter size: 3, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.1436\n",
      "Training with filters: 32, filter size: 3, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0431\n",
      "Training with filters: 32, filter size: 5, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: -0.0096\n",
      "Training with filters: 32, filter size: 5, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: -0.0343\n",
      "Training with filters: 32, filter size: 7, hidden dim: 64\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0673\n",
      "Training with filters: 32, filter size: 7, hidden dim: 128\n",
      "\n",
      "Final Sharpe Ratio on Validation Set: 0.0124\n",
      "Best parameters: (8, 5, 128) with Sharpe Ratio on Validation Set: 0.15007934907048204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MAIN FUNCTION\n",
    "### -------------- PARSING DATA -------------- ###\n",
    "parser = DataLoader(file_path=\"../data/european_wholesale_electricity_price_data_daily.csv\")\n",
    "\n",
    "# Get list of countries\n",
    "print(\"\\n--- List of Countries ---\")\n",
    "all_countries_list = parser.get_country_list()\n",
    "print(all_countries_list)\n",
    "\n",
    "# Get daily price matrix for all countries for the entire year 2021\n",
    "price_matrix = parser.get_price_matrix(\n",
    "    time_range=\"2015-01-01,2024-12-31\",\n",
    "    countries=all_countries_list,\n",
    "    fill_method=\"ffill\"\n",
    ")\n",
    "\n",
    "# Get the raw daily returns for the price matrix\n",
    "returns = price_matrix.pct_change().dropna()\n",
    "\n",
    "\n",
    "\n",
    "### -------------- COINTEGRATION RESIDUALS -------------- ###\n",
    "# Create an instance of the CointegrationResidualGenerator\n",
    "residual_generator = CointegrationResidualGenerator(price_matrix)\n",
    "\n",
    "residual_generator.compute_all_asset_residuals()\n",
    "\n",
    "# Get residuals\n",
    "asset_residuals = residual_generator.get_asset_residuals()\n",
    "\n",
    "# Get the input for CNN\n",
    "# cnn_input contains a set of 329 data samples, each sample represents 30-day cumulative residuals for the 31 countries\n",
    "cumulative_residual_window = 30\n",
    "cnn_input = residual_generator.prepare_cnn_input_from_residuals(window=cumulative_residual_window)\n",
    "\n",
    "# Get the start index of the first 30-day cumulative residuals in the returns DataFrame\n",
    "start_idx_in_returns = returns.index.get_loc(asset_residuals.index[0])\n",
    "num_samples = len(asset_residuals) - cumulative_residual_window + 1\n",
    "next_day_indices = [start_idx_in_returns + i + cumulative_residual_window for i in range(num_samples)]\n",
    "\n",
    "# Get the next-day returns for the corresponding indices\n",
    "# The next-day returns are the returns for the day after the last day of each 30-day window\n",
    "# For example, if the first 30-day window ends on index 0, the next day return is at index 1\n",
    "# If the second 30-day window ends on index 1, the next day return is at index 2, and so on.\n",
    "next_day_returns = returns.iloc[next_day_indices]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### -------------- FEED THE 30-DAY CUMULATIVE RESIDUALS OF EVERY COUNTRY TO CNN+FFN -------------- ###\n",
    "# Transform cnn_input to be compatible with the CNN input shape\n",
    "cnn_input_array = cnn_input.transpose(0, 2, 1) # [samples, features, window]\n",
    "\n",
    "# FILIP'S SECTION: CNN+FFN\n",
    "# Hey Filip, this is the section where you can add your code to train the CNN+FFN model.\n",
    "# cnn_input_array essentially contains 329 training data points, each data point is 30-day cumulative residuals for the 31 countries.\n",
    "# So, for one \"data point\" you would feed the set of 30-day cumulative residuals for every country to the CNN+FFN model.\n",
    "# One data point should result in a set of weights for each of the 31 countries.\n",
    "# Each set of weights is used to calculate one next-day portfolio return.\n",
    "# Repeat this for all 329 data points to get a set of 329 portfolio returns.\n",
    "# You can then use these portfolio returns to calculate the Sharpe ratio\n",
    "# Optimize the CNN+FFN to maximize the Sharpe ratio.\n",
    "\n",
    "# Set the device to GPU if available, otherwise MPS (for Mac silicon) or CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.manual_seed(1)  # For reproducibility\n",
    "\n",
    "# Create the PortfolioOptimizer instance and train the model\n",
    "# NOTE: We need to have some validation data to evaluate the model performance\n",
    "# otherwise we will be overfitting the model to the training data\n",
    "\n",
    "# Grid search for hyperparameters\n",
    "num_filters = [8, 16, 32]\n",
    "filter_sizes = [3, 5, 7]\n",
    "hidden_dims = [64, 128]\n",
    "\n",
    "# Choose the best hyperparameters based on validation performance\n",
    "best_score = float('-inf')\n",
    "best_params = None\n",
    "\n",
    "# Training and validation split\n",
    "# 70% training, 15% validation, 15% testing\n",
    "# Note: The test set is not used in the training process, but it can be used ONCE to evaluate the final model performance\n",
    "# The validation set is used to tune the hyperparameters and avoid overfitting\n",
    "\"\"\"\n",
    "train_dataset = cnn_input_array[:int(0.7 * len(cnn_input_array))]\n",
    "train_next_day_returns = next_day_returns[:int(0.7 * len(next_day_returns))]\n",
    "\n",
    "val_dataset = cnn_input_array[int(0.7 * len(cnn_input_array)):]\n",
    "val_next_day_returns = next_day_returns[int(0.7 * len(next_day_returns)):]\n",
    "\n",
    "test_dataset = val_dataset[int(0.5 * len(val_dataset)):]\n",
    "test_next_day_returns = val_next_day_returns[int(0.5 * len(val_next_day_returns)):]\n",
    "\"\"\"\n",
    "datasetSplitter = TimeSeriesDataSplitter(data_x=cnn_input_array, data_y=next_day_returns, train_size=0.7, validation_size=0.15, test_size=0.15)\n",
    "(train), (val), (test) = datasetSplitter.split_data()\n",
    "\n",
    "# Unpack the datasets\n",
    "train_dataset, train_next_day_returns = train\n",
    "val_dataset, val_next_day_returns = val\n",
    "test_dataset, test_next_day_returns = test\n",
    "\n",
    "print(f\"Train dataset shape: {train_dataset.shape}, Train next day returns shape: {train_next_day_returns.shape}\")\n",
    "print(f\"Validation dataset shape: {val_dataset.shape}, Validation next day returns shape: {val_next_day_returns.shape}\")\n",
    "print(f\"Test dataset shape: {test_dataset.shape}, Test next day returns shape: {test_next_day_returns.shape}\")\n",
    "\n",
    "for num_filter in num_filters:\n",
    "    for filter_size in filter_sizes:\n",
    "        for hidden_dim in hidden_dims:\n",
    "                print(f\"Training with filters: {num_filter}, filter size: {filter_size}, hidden dim: {hidden_dim}\")\n",
    "                \n",
    "                # Initialize the PortfolioOptimizer\n",
    "                optimizer = PortfolioOptimizer(train_dataset, \n",
    "                                               train_next_day_returns, \n",
    "                                               val_dataset,\n",
    "                                               val_next_day_returns,\n",
    "                                               batch_size=1000, \n",
    "                                               num_epochs=1000, \n",
    "                                               num_filters=num_filter, \n",
    "                                               device=device, \n",
    "                                               filter_size=filter_size,\n",
    "                                               hidden_dim=hidden_dim)\n",
    "                \n",
    "                # Train the model\n",
    "                _, portfolio_returns = optimizer.train(verbose=False) # The final sharpe should be from the validation set\n",
    "\n",
    "                # Evaluate the model on the validation set\n",
    "                val_sharpe, _ = optimizer.evaluate_final()\n",
    "                if val_sharpe > best_score:\n",
    "                    best_score = val_sharpe\n",
    "                    best_params = (num_filter, filter_size, hidden_dim)\n",
    "                    best_portfolio_returns = portfolio_returns\n",
    "    \n",
    "print(f\"Best parameters: {best_params} with Sharpe Ratio on Validation Set: {best_score}\")\n",
    "\n",
    "### -------------- GET THE WEIGHTS OUTPUTTED FROM CNN+FFN -------------- ###\n",
    "# Initializing the Sharpe ratio evaluator\n",
    "#evaluator = BacktestSharpeEvaluator()\n",
    "\n",
    "# Get the weight outputted from the CNN+FFN model\n",
    "# One weight for each country\n",
    "#weights = np.array([]) # CHANGE THIS TO THE ACTUAL WEIGHTS OUTPUTTED FROM THE CNN+FFN MODEL\n",
    "\n",
    "# Normalize the weights using L1 normalization\n",
    "# This is done to ensure that the portfolio is dollar-neutral\n",
    "#normalized_weights = evaluator.normalize_weights_l1(weights)\n",
    "\n",
    "# Multiply the normalized weights (vector) with the next-day returns (vector) to get the portfolio return\n",
    "#next_day_portfolio_return = evaluator.compute_portfolio_return(normalized_weights, next_day_returns.iloc[0].values)\n",
    "\n",
    "# Store the next day portfolio return \n",
    "#evaluator.add_return(next_day_portfolio_return)\n",
    "\n",
    "# Repeat the above step for all 329 data points, adding the portfolio returns to the evaluator for each data point\n",
    "\n",
    "# Once all portfolio returns are calculated, you can calculate the Sharpe ratio\n",
    "\n",
    "# Train the model to optimize the Sharpe Ratio\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
