{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731eb1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from typing import Optional, List\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Parses European wholesale electricity price data, allowing filtering\n",
    "    by country and date range.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path=\"./data/european_wholesale_electricity_price_data_daily.csv\"):\n",
    "        \"\"\"\n",
    "        Initializes the parser and loads the data.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the CSV file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Loads and preprocesses the data from the CSV file.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.file_path)\n",
    "            # Convert 'Date' column to datetime objects\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            # Drop ISO3 Code column\n",
    "            df.drop(columns={'ISO3 Code'}, inplace=True)\n",
    "            # Rename price column for easier access\n",
    "            df.rename(columns={'Price (EUR/MWhe)': 'Price'}, inplace=True)\n",
    "            print(f\"Data loaded successfully from {self.file_path}\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found at {self.file_path}\")\n",
    "            return None\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: Expected column '{e}' not found in the CSV.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing file: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_data_by_country_and_range(self, time_range:str, country=None):\n",
    "        \"\"\"\n",
    "        Filters the data for a specific country and time range.\n",
    "\n",
    "        Args:\n",
    "            country (str): The name of the country to filter by (e.g., 'Germany').\n",
    "            time_range (str): A string representing the date range in the format\n",
    "                              'YYYY-MM-DD,YYYY-MM-DD'.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame containing the filtered data,\n",
    "                              or None if an error occurs or no data is found.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"Error: Data not loaded.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            start_date_str, end_date_str = time_range.split(',')\n",
    "            start_date = pd.to_datetime(start_date_str.strip())\n",
    "            end_date = pd.to_datetime(end_date_str.strip())\n",
    "        except ValueError:\n",
    "            print(\"Error: Invalid time_range format. Please use 'YYYY-MM-DD,YYYY-MM-DD'.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "             print(f\"Error parsing time range: {e}\")\n",
    "             return None\n",
    "\n",
    "        outputData = self.data.copy()\n",
    "        # If a country is specified, filter the data by country, if not use all data\n",
    "        if country is not None:\n",
    "            outputData = self.data[self.data['Country'].str.lower() == country.lower()]\n",
    "\n",
    "        # Filter by date range (inclusive)\n",
    "        filtered_data = outputData[\n",
    "            (outputData['Date'] >= start_date) & (outputData['Date'] <= end_date)\n",
    "        ]\n",
    "\n",
    "        if filtered_data.empty:\n",
    "            print(f\"Warning: No data found for country '{country}' within the range {time_range}.\")\n",
    "            return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "        return filtered_data.copy() # Return a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "    def get_all_data(self):\n",
    "        \"\"\"\n",
    "        Returns the entire dataset.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: The entire dataset.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"Error: Data not loaded.\")\n",
    "            return None\n",
    "        return self.data.copy()\n",
    "\n",
    "    def get_country_list(self):\n",
    "        \"\"\"\n",
    "        Returns a list of unique countries in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of unique country names.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            print(\"Error: Data not loaded.\")\n",
    "            return None\n",
    "        return self.data['Country'].unique().tolist()\n",
    "    \n",
    "    def get_price_matrix(\n",
    "        self,\n",
    "        time_range: str,\n",
    "        countries: List[str],\n",
    "        fill_method: Optional[str] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a price matrix where:\n",
    "        - Rows = dates\n",
    "        - Columns = countries\n",
    "        - Values = daily electricity prices\n",
    "\n",
    "        Parameters:\n",
    "        - time_range (str): e.g. \"2021-05-10,2021-05-16\"\n",
    "        - countries (List[str]): list of country names to include\n",
    "        - fill_method (Optional[str]): 'ffill', 'bfill', or None\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: index=date, columns=country names, values=prices\n",
    "        \"\"\"\n",
    "        start_date, end_date = time_range.split(\",\")\n",
    "\n",
    "        # Filter the master data once\n",
    "        df = self.data.copy()\n",
    "        df = df[df[\"Country\"].isin(countries)]\n",
    "        df = df[(df[\"Date\"] >= start_date) & (df[\"Date\"] <= end_date)]\n",
    "\n",
    "        # Pivot: index=date, columns=country, values=price\n",
    "        price_matrix = df.pivot(index=\"Date\", columns=\"Country\", values=\"Price\").sort_index()\n",
    "\n",
    "        # Handle missing data\n",
    "        if fill_method:\n",
    "            price_matrix = price_matrix.fillna(method=fill_method)\n",
    "        else:\n",
    "            price_matrix = price_matrix.dropna()\n",
    "\n",
    "        return price_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CointegrationResidualGenerator:\n",
    "    def __init__(self, price_data: pd.DataFrame, risk_free_rate_annual: float = 0.01):\n",
    "        self.price_data = price_data\n",
    "        self.risk_free_rate_daily = risk_free_rate_annual / 252\n",
    "        self.returns = self._compute_excess_returns(price_data)\n",
    "        self.cumulative_returns = self._compute_cumulative_returns(self.returns)\n",
    "        self.asset_residuals = pd.DataFrame(index=self.cumulative_returns.index)\n",
    "        self.betas = {}  # Store betas for each asset\n",
    "\n",
    "    def _compute_excess_returns(self, price_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Computes daily excess log returns.\"\"\"\n",
    "        log_returns = np.log(price_data / price_data.shift(1)).dropna()\n",
    "        excess_returns = log_returns.subtract(self.risk_free_rate_daily)\n",
    "        return excess_returns\n",
    "\n",
    "    def _compute_cumulative_returns(self, returns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Computes cumulative returns.\"\"\"\n",
    "        cumulative_returns = returns.cumsum()\n",
    "        return cumulative_returns\n",
    "\n",
    "    def compute_all_asset_residuals(self):\n",
    "        \"\"\"Computes residuals for each asset treated as dependent variable.\"\"\"\n",
    "        for target_asset in self.cumulative_returns.columns:\n",
    "            # Set current asset as dependent variable\n",
    "            y = self.cumulative_returns[target_asset].values.reshape(-1, 1)\n",
    "            X = self.cumulative_returns.drop(columns=[target_asset]).values\n",
    "            X_cols = self.cumulative_returns.drop(columns=[target_asset]).columns\n",
    "\n",
    "            # Fit linear regression: y ~ X\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            betas = model.coef_[0]\n",
    "            intercept = model.intercept_[0]\n",
    "\n",
    "            # Predict values and compute residuals for this asset\n",
    "            y_pred = model.predict(X).flatten()\n",
    "            residuals = y.flatten() - y_pred\n",
    "\n",
    "            # Store residuals in DataFrame\n",
    "            self.asset_residuals[target_asset] = residuals\n",
    "            # Store betas for this asset\n",
    "            beta_series = pd.Series(betas, index=X_cols)\n",
    "            beta_series['Intercept'] = intercept\n",
    "            self.betas[target_asset] = beta_series\n",
    "\n",
    "    def get_asset_residuals(self) -> pd.DataFrame:\n",
    "        \"\"\"Returns residuals for all assets.\"\"\"\n",
    "        if self.asset_residuals.empty:\n",
    "            raise ValueError(\"Asset residuals not yet computed.\")\n",
    "        return self.asset_residuals\n",
    "\n",
    "    def get_betas_for_asset(self, asset: str) -> pd.Series:\n",
    "        \"\"\"Returns betas used to form residuals for a specific asset.\"\"\"\n",
    "        if asset not in self.betas:\n",
    "            raise ValueError(f\"Betas for asset '{asset}' not found. Compute residuals first.\")\n",
    "        return self.betas[asset]\n",
    "\n",
    "    def prepare_cnn_input_from_residuals(self, window: int = 30):\n",
    "        \"\"\"\n",
    "        Prepares CNN input data by creating rolling cumulative residuals.\n",
    "        \n",
    "        Returns:\n",
    "        - 3D numpy array: [samples, window, assets]\n",
    "        \"\"\"\n",
    "        if self.asset_residuals.empty:\n",
    "            raise ValueError(\"Asset residuals not yet computed.\")\n",
    "\n",
    "        cnn_input_list = []\n",
    "\n",
    "        for start_idx in range(len(self.asset_residuals) - window + 1):\n",
    "            # Slice window of residuals\n",
    "            window_residuals = self.asset_residuals.iloc[start_idx:start_idx + window]\n",
    "            # Cumulative sum within the window\n",
    "            cumulative_window = window_residuals.cumsum()\n",
    "            # Store the result\n",
    "            cnn_input_list.append(cumulative_window.values)\n",
    "\n",
    "        # Convert to 3D numpy array: [samples, window, assets]\n",
    "        cnn_input_array = np.array(cnn_input_list)\n",
    "        return cnn_input_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81b1826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_length, num_features, num_filters= 8, num_classes=2, filter_size=2):\n",
    "        \"\"\"\n",
    "        Initialize the CNN model based on the equations in the paper.\n",
    "        \n",
    "        Args:\n",
    "            input_length (int): Length of the input sequence (L in the equations)\n",
    "            num_features (int): Number of features per time step\n",
    "            num_classes (int): Number of output classes\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.num_filters = num_filters  # Number of filters (D = 8 according to equation)\n",
    "        self.filter_size = filter_size  # Filter size (filter_size = 2 according to equation)\n",
    "        \n",
    "        # First convolutional layer (Equation 3)\n",
    "        # Input shape: [batch_size, num_features, input_length]\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=num_features,\n",
    "            out_channels=self.num_filters,\n",
    "            kernel_size=self.filter_size,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        # Second convolutional layer (Equation 4)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=self.num_filters,\n",
    "            out_channels=self.num_filters,\n",
    "            kernel_size=self.filter_size,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        # Calculate output sizes after convolutions\n",
    "        L_after_conv1 = input_length - self.filter_size + 1\n",
    "        L_after_conv2 = L_after_conv1 - self.filter_size + 1\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(self.num_filters * L_after_conv2, num_classes)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, num_features, input_length]\n",
    "                             Represents the x_i(t) vector in Equation 2\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions\n",
    "        \"\"\"\n",
    "        # Store original input for skip connection (Equation 5)\n",
    "        x_original = x\n",
    "        \n",
    "        # First convolutional layer (Equation 3)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        \n",
    "        # Second convolutional layer (Equation 4)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Skip connection (Equation 5)\n",
    "        # Need to adjust dimensions for skip connection\n",
    "        # Cut or pad x_original to match x dimensions\n",
    "        if x_original.shape[2] > x.shape[2]:\n",
    "            # If original is longer, cut it\n",
    "            diff = x_original.shape[2] - x.shape[2]\n",
    "            x_skip = x_original[:, :, diff:]\n",
    "        else:\n",
    "            # If original is shorter, this would require padding\n",
    "            # For simplicity, we'll just use the original shape\n",
    "            x_skip = x_original\n",
    "        \n",
    "        # Apply the skip connection if dimensions match\n",
    "        if x.shape == x_skip.shape:\n",
    "            x = x + x_skip\n",
    "        \n",
    "        # Flatten and pass through fully connected layer\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e620c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # building blocks for neural networks\n",
    "import torch.nn.functional as F # access to functions like ReLU, sigmoid, etc.\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    # IDEA: maybe try other relu variants such as LeakyReLU, ELU, etc.\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        w_eps = self.out(x) # equation (10), single scalar value for each input\n",
    "        return w_eps\n",
    "    \n",
    "# equation (11) from project framework\n",
    "def soft_normalize(weights):\n",
    "    \"\"\"\n",
    "    Normalize allocation weights using L1 norm (sum of absolute values).\n",
    "    weights: Tensor of shape [batch_size, 1]\n",
    "    Returns: Normalized weights of shape [batch_size, 1]\n",
    "    \"\"\"\n",
    "    l1_norm = torch.sum(torch.abs(weights), dim=0, keepdim=True) + 1e-8 # avoid division by zero\n",
    "    normalized_weights = weights / l1_norm\n",
    "    return normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2a072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from ./data/european_wholesale_electricity_price_data_daily.csv\n",
      "\n",
      "--- List of Countries ---\n",
      "['Austria', 'Belgium', 'Czechia', 'Denmark', 'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Italy', 'Latvia', 'Lithuania', 'Luxembourg', 'Netherlands', 'Norway', 'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland', 'United Kingdom', 'Bulgaria', 'Serbia', 'Croatia', 'Montenegro', 'North Macedonia', 'Ireland']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Volter\\AppData\\Local\\Temp\\ipykernel_15748\\797593302.py:142: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  price_matrix = price_matrix.fillna(method=fill_method)\n",
      "c:\\Users\\Volter\\anaconda3\\envs\\DeepLearningSTAT7007\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MAIN FUNCTION\n",
    "### -------------- PARSING DATA -------------- ###\n",
    "parser = DataLoader()\n",
    "\n",
    "# Get list of countries\n",
    "print(\"\\n--- List of Countries ---\")\n",
    "all_countries_list = parser.get_country_list()\n",
    "print(all_countries_list)\n",
    "\n",
    "# Get daily price matrix for all countries for the entire year 2021\n",
    "price_matrix = parser.get_price_matrix(\n",
    "    time_range=\"2021-01-01,2021-12-31\",\n",
    "    countries=all_countries_list,\n",
    "    fill_method=\"ffill\"\n",
    ")\n",
    "\n",
    "# Get the raw daily returns for the price matrix\n",
    "returns = price_matrix.pct_change().dropna()\n",
    "\n",
    "# # Plot the price vs time, and the daily returns for the first country in the list, enable grid\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(price_matrix.index, price_matrix.iloc[:, 0], label=price_matrix.columns[0])\n",
    "# plt.title(\"Price vs Time\")\n",
    "# plt.xlabel(\"Date\")\n",
    "# plt.ylabel(\"Price\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(returns.index, returns.iloc[:, 0], label=returns.columns[0])\n",
    "# plt.title(\"Daily Returns vs Time\")\n",
    "# plt.xlabel(\"Date\")\n",
    "# plt.ylabel(\"Daily Returns\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "### -------------- COINTEGRATION RESIDUALS -------------- ###\n",
    "# Create an instance of the CointegrationResidualGenerator\n",
    "residual_generator = CointegrationResidualGenerator(price_matrix)\n",
    "\n",
    "residual_generator.compute_all_asset_residuals()\n",
    "\n",
    "# Get residuals\n",
    "asset_residuals = residual_generator.get_asset_residuals()\n",
    "\n",
    "# Get the input for CNN\n",
    "# cnn_input contains a set of 329 data samples, each sample represents 30-day cumulative residuals for the 31 countries\n",
    "cumulative_residual_window = 30\n",
    "cnn_input = residual_generator.prepare_cnn_input_from_residuals(window=cumulative_residual_window)\n",
    "\n",
    "# Get the next-day returns corresponding to each of the 30-day cumulative residuals as a DataFrame\n",
    "next_day_returns = returns.iloc[cumulative_residual_window + 1:]\n",
    "\n",
    "# Get the start index of the first 30-day cumulative residuals in the returns DataFrame\n",
    "start_idx_in_returns = returns.index.get_loc(asset_residuals.index[0])\n",
    "num_samples = len(asset_residuals) - cumulative_residual_window + 1\n",
    "next_day_indices = [start_idx_in_returns + i + cumulative_residual_window for i in range(num_samples)]\n",
    "next_day_returns = returns.iloc[next_day_indices]\n",
    "\n",
    "\n",
    "### -------------- FEED THE 30-DAY CUMULATIVE RESIDUALS OF EVERY COUNTRY TO CNN+FNN -------------- ###\n",
    "# Transform cnn_input to be compatible with the CNN input shape\n",
    "cnn_input_array = cnn_input.transpose(0, 2, 1) # [samples, features, window]\n",
    "\n",
    "# FILIP'S SECTION: CNN+FNN\n",
    "# Hey Filip, this is the section where you can add your code to train the CNN+FNN model.\n",
    "# cnn_input_array essentially contains 329 training data points, each data point is 30-day cumulative residuals for the 31 countries.\n",
    "# So, for one \"data point\" you would feed the set of 30-day cumulative residuals for every country to the CNN+FNN model.\n",
    "# One data point should result in a set of weights for each of the 31 countries.\n",
    "# Each set of weights is used to calculate one next-day portfolio return.\n",
    "# Repeat this for all 329 data points to get a set of 329 portfolio returns.\n",
    "# You can then use these portfolio returns to calculate the Sharpe ratio\n",
    "# Optimize the CNN+FFN to maximize the Sharpe ratio.\n",
    "\n",
    "\n",
    "\n",
    "### -------------- GET THE WEIGHTS OUTPUTTED FROM CNN+FNN -------------- ###\n",
    "# Get the weight outputted from the CNN+FNN model\n",
    "# One weight for each country\n",
    "weights = [] # Change this to the actual weights outputted from the CNN+FNN model\n",
    "\n",
    "# Convert weights to  \n",
    "\n",
    "\n",
    "# next_day_returns = np.array([0.01, 0.02, -0.01])\n",
    "# normalized_weights = evaluator.normalize_weights_l1(raw_weights)\n",
    "# next_day_portfolio_return = evaluator.compute_portfolio_return(raw_weights, next_day_returns)\n",
    "# # Store the next day portfolio return\n",
    "# evaluator.add_return(next_day_portfolio_return)\n",
    "# print(\"Next Day Portfolio Return:\", next_day_portfolio_return)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningSTAT7007",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
