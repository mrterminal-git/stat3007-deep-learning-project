{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf29b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        pool: str = 'last'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transformer module for time-series features output by CNN.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of feature embeddings (num_filters from CNN).\n",
    "            nhead (int): Number of attention heads.\n",
    "            num_layers (int): Number of Transformer encoder layers.\n",
    "            dropout (float): Dropout probability in Transformer.\n",
    "            pool (str): Pooling strategy: 'last' or 'mean'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.pool = pool\n",
    "\n",
    "    def forward(self, cnn_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cnn_features: Tensor of shape [batch_size, d_model, seq_len]\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, d_model]\n",
    "        \"\"\"\n",
    "        # Permute to [seq_len, batch_size, d_model]\n",
    "        x = cnn_features.permute(2, 0, 1)\n",
    "        # Apply Transformer encoder\n",
    "        tr_out = self.transformer(x)  # [seq_len, batch_size, d_model]\n",
    "        # Pool over the sequence dimension\n",
    "        if self.pool == 'last':\n",
    "            output = tr_out[-1]         # Last time step [batch_size, d_model]\n",
    "        elif self.pool == 'mean':\n",
    "            output = tr_out.mean(dim=0) # Mean over seq [batch_size, d_model]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pool type: {self.pool}\")\n",
    "        return output\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7b213",
   "metadata": {},
   "source": [
    "Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # force CPU\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "\n",
    "    # 1) Synthetic price data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(\"2022-01-01\", periods=10)\n",
    "    prices = pd.DataFrame({\n",
    "        \"Asset_1\": np.cumprod(1 + np.random.normal(0, 0.01, size=10)),\n",
    "        \"Asset_2\": np.cumprod(1 + np.random.normal(0, 0.01, size=10))\n",
    "    }, index=dates)\n",
    "    print(\"1. Prices shape:\", prices.shape, \"\\n\")\n",
    "\n",
    "    # 2) Generate residuals\n",
    "    gen = CointegrationResidualGenerator(prices)\n",
    "    gen.compute_all_asset_residuals()\n",
    "    residuals = gen.get_asset_residuals()\n",
    "    print(\"2. Residuals shape:\", residuals.shape, \"\\n\")\n",
    "\n",
    "    # 3) Prepare CNN input\n",
    "    window = 5\n",
    "    cnn_np = gen.prepare_cnn_input_from_residuals(window=window)\n",
    "    print(\"3. CNN input shape:\", cnn_np.shape, \"\\n\")\n",
    "    cnn_tensor = torch.tensor(\n",
    "        cnn_np.transpose(0,2,1),\n",
    "        dtype=torch.float32,\n",
    "        device='cpu'\n",
    "    )\n",
    "\n",
    "    # 4) CNN forward (handle 1‐ or 2‐output cases)\n",
    "    cnn_model = CNN(\n",
    "        input_length=window,\n",
    "        num_features=cnn_np.shape[2],\n",
    "        num_filters=4,\n",
    "        num_classes=2,\n",
    "        filter_size=2\n",
    "    ).to('cpu')\n",
    "\n",
    "    cnn_out = cnn_model(cnn_tensor)\n",
    "    if isinstance(cnn_out, tuple):\n",
    "        logits, feat = cnn_out\n",
    "    else:\n",
    "        # logits only; manually extract the conv‐feature map for Transformer\n",
    "        logits = cnn_out\n",
    "        with torch.no_grad():\n",
    "            x1 = cnn_model.relu(cnn_model.conv1(cnn_tensor))\n",
    "            feat = cnn_model.relu(cnn_model.conv2(x1))\n",
    "            # apply skip‐connection exactly as in your CNN\n",
    "            diff = cnn_tensor.shape[2] - feat.shape[2]\n",
    "            x_skip = cnn_tensor[:, :, diff:] if diff>0 else cnn_tensor\n",
    "            if feat.shape == x_skip.shape:\n",
    "                feat = feat + x_skip\n",
    "\n",
    "    print(\"4. CNN logits shape:\", logits.shape)\n",
    "    print(\"5. CNN feature map shape:\", feat.shape, \"\\n\")\n",
    "\n",
    "    # 5) Transformer forward\n",
    "    trans_model = TimeSeriesTransformer(\n",
    "        d_model=feat.size(1),\n",
    "        nhead=4\n",
    "    ).to('cpu')\n",
    "    emb = trans_model(feat)\n",
    "    print(\"6. Transformer embedding shape:\", emb.shape, \"\\n\")\n",
    "\n",
    "    # 6) FNN allocator + normalization\n",
    "    fnn_model = FNN(input_dim=emb.size(1), hidden_dim=32).to('cpu')\n",
    "    w_eps  = fnn_model(emb)\n",
    "    w_norm = soft_normalize(w_eps)\n",
    "    print(\"7. Raw weights shape:\", w_eps.shape)\n",
    "    print(\"8. Normalized weights shape:\", w_norm.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec34d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Display the price matrix\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"1) Electricity Price Matrix (France & Germany, 2021-05-10→16):\")\n",
    "display(price_matrix.head())\n",
    "print(\"Shape:\", price_matrix.shape, \"\\n\")\n",
    "\n",
    "# 2) Generate cointegration residuals\n",
    "gen = CointegrationResidualGenerator(price_matrix)\n",
    "gen.compute_all_asset_residuals()\n",
    "residuals = gen.get_asset_residuals()\n",
    "\n",
    "print(\"2) Cointegration Residuals (head):\")\n",
    "display(residuals.head())\n",
    "print(\"Shape:\", residuals.shape, \"\\n\")\n",
    "\n",
    "# 3) Prepare CNN input (3-day window for this tiny example)\n",
    "window = 3\n",
    "cnn_np = gen.prepare_cnn_input_from_residuals(window=window)\n",
    "print(f\"3) CNN input shape: {cnn_np.shape}  # (windows, days, assets)\")\n",
    "print(\"   Sample window [0]:\\n\", cnn_np[0], \"\\n\")\n",
    "\n",
    "# 4) Run through CNN block\n",
    "import torch\n",
    "\n",
    "# Build tensor [batch, features, length]\n",
    "cnn_tensor = torch.tensor(cnn_np.transpose(0, 2, 1), dtype=torch.float32)\n",
    "\n",
    "cnn_model = CNN(\n",
    "    input_length=window,\n",
    "    num_features=cnn_np.shape[2],\n",
    "    num_filters=4,\n",
    "    num_classes=2,\n",
    "    filter_size=2\n",
    ").to(\"cpu\")\n",
    "\n",
    "# Forward pass\n",
    "out = cnn_model(cnn_tensor)\n",
    "# Handle return signature\n",
    "if isinstance(out, tuple):\n",
    "    logits, feat = out\n",
    "else:\n",
    "    logits = out\n",
    "    # Manually rebuild feat if needed\n",
    "    x1 = cnn_model.relu(cnn_model.conv1(cnn_tensor))\n",
    "    feat = cnn_model.relu(cnn_model.conv2(x1))\n",
    "    diff = cnn_tensor.size(2) - feat.size(2)\n",
    "    skip = cnn_tensor[:, :, diff:] if diff > 0 else cnn_tensor\n",
    "    if feat.shape == skip.shape:\n",
    "        feat = feat + skip\n",
    "\n",
    "print(f\"4) CNN logits shape: {logits.shape}\")\n",
    "print(f\"   Sample logits:\\n{logits[:3]}\\n\")\n",
    "print(f\"5) CNN feature-map shape: {feat.shape}\\n\")\n",
    "\n",
    "# 5) Transformer\n",
    "trans_model = TimeSeriesTransformer(d_model=feat.size(1), nhead=4).to(\"cpu\")\n",
    "emb = trans_model(feat)\n",
    "print(f\"6) Transformer embeddings shape: {emb.shape}\")\n",
    "print(f\"   Sample embeddings:\\n{emb[:3]}\\n\")\n",
    "\n",
    "# 6) FNN allocator + soft-normalize\n",
    "fnn_model = FNN(input_dim=emb.size(1), hidden_dim=32).to(\"cpu\")\n",
    "w_raw = fnn_model(emb)\n",
    "w_norm = soft_normalize(w_raw)\n",
    "\n",
    "print(f\"7) Raw allocation weights shape: {w_raw.shape}\")\n",
    "print(f\"   Sample w_raw:\\n{w_raw[:3]}\\n\")\n",
    "print(f\"8) L₁-normalized weights shape: {w_norm.shape}\")\n",
    "print(f\"   Sample w_norm:\\n{w_norm[:3]}\")\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
